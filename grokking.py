# -*- coding: utf-8 -*-
"""GROKKING

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CD8FP4CTLyWDdJANiir0srC7w1gyUYn_

#ENVIRONMENT SETUP

**Installations**
"""

!pip install transformer-lens
!pip install --upgrade torch
!pip install gdown --quiet

"""**Libraries**"""

import einops
import tqdm.auto as tqdm
import copy
import matplotlib.pyplot as plt
import numpy as np
from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache
import torch
import os
import gdown
import torch.nn.utils as nn_utils
from torch.optim import AdamW
import seaborn as sns
from torch.utils.tensorboard import SummaryWriter
from google.colab import drive

"""**Custom library setup**.  *To encapsulate variables and methods, maintaining a static experimental setup*"""

drive.mount('/content/drive')
file_id = "1wCC_0AnnizoyBRcmbKaj1hTmd_vLPtGi"
destination = "grokking.py"
gdown.download(f"https://drive.google.com/uc?id={file_id}", destination, quiet=False)

"""**GPU setup**"""

# MUST happen before importing torch
!nvcc --version        # Check CUDA toolkit version
!python -c "import torch; print(torch.version.cuda)"

os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
torch.cuda.empty_cache()  # clear GPU cache

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

torch.backends.cudnn.deterministic = False
torch.backends.cudnn.benchmark = False

"""#TRAINING METHODOLOGY
*with different regularization methods*

**Importing from the custom made library**.   *We are working with* (a+b)%113 *dataset*

Dataset variables:            
*   p = 113  
*   frac_train = 0.3 -> *Fraction of data used for training*
* lr = 1e-3 -> *Learning rate*
* wd = 1.0 -> *Weight decay*
* betas = (0.9, 0.98) -> *AdamW betas*
* num_epochs = 25000 -> *Total number of training epochs*
* checkpoint_every = 1000
* DATA_SEED = 598
"""

from grokking import (
    p, frac_train, lr, wd, betas,
    num_epochs, checkpoint_every, DATA_SEED,
    make_mod113_dataset, build_model_config,
    freeze_biases, loss_fn, visualize_losses
)

"""
**Regularization** : None

"""

def main():



    # Create the dataset
    train_data, train_labels, test_data, test_labels = make_mod113_dataset(device)

    # Build the model config and instantiate the model
    cfg = build_model_config(device)
    model = HookedTransformer(cfg).to(device)
    num_layers = len(model.blocks)
    print(f"Number of Layers in the Model: {num_layers}")

    # freeze biases -optional
    freeze_biases(model)

    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=betas)

    # Training loop
    train_losses = []
    test_losses = []
    model_checkpoints = []
    checkpoint_epochs = []

    for epoch in tqdm.tqdm(range(num_epochs)):
        # Train step
        model.train()
        logits_train = model(train_data)  # shape (batch, seq=3, d_vocab_out)
        train_loss_val = loss_fn(logits_train, train_labels)
        train_loss_val.backward()
        train_losses.append(train_loss_val.item())

        optimizer.step()
        optimizer.zero_grad()

        # Evaluation step
        model.eval()
        with torch.inference_mode():
            logits_test = model(test_data)
            test_loss_val = loss_fn(logits_test, test_labels)
            test_losses.append(test_loss_val.item())

        # Checkpoint
        if (epoch + 1) % checkpoint_every == 0:
            checkpoint_epochs.append(epoch)
            model_checkpoints.append(copy.deepcopy(model.state_dict()))
            print(f"Epoch {epoch} "
                  f"Train Loss: {train_loss_val.item():.4f} "
                  f"Test Loss: {test_loss_val.item():.4f}")

    # Save final model
    PATH = "./grokking_model.pth"
    torch.save(
        {
            "model": model.state_dict(),
            "config": model.cfg,
            "checkpoints": model_checkpoints,
            "checkpoint_epochs": checkpoint_epochs,
            "test_losses": test_losses,
            "train_losses": train_losses,
        },
        PATH
    )
    print(f"Model and losses saved to {PATH}")

    # Optionally reload the saved model to verify
    saved_data = torch.load(PATH, map_location=device)
    cfg_loaded = saved_data["config"]
    model_loaded = HookedTransformer(cfg_loaded).to(device)
    model_loaded.load_state_dict(saved_data["model"])

    # Visualization using our module
    visualize_losses(
        train_losses=saved_data["train_losses"],
        test_losses=saved_data["test_losses"],
        clamp=15,
        title="(a+b)%113 - Training and Test Losses"
    )

if __name__ == "__main__":
    main()

def visualize_model_from_checkpoint(
    checkpoint_path,
    show_plots=True,
    save_plots=False,
    normalize=False,
    colormap="viridis",
):
    """
    Visualize embeddings, attention weights, and analyze sparsity from a single checkpoint.
    """

    def normalize_matrix(matrix):
        """Normalize a matrix for visualization."""
        return matrix / matrix.max() if normalize else matrix

    def calculate_sparsity(matrix, threshold=1e-3):
        """Calculate the fraction of weights below a threshold."""
        return (np.abs(matrix) < threshold).mean()

    try:
        # Load the saved model and config
        ckpt = torch.load(checkpoint_path, map_location="cpu")
        if "model" not in ckpt or "config" not in ckpt:
            raise ValueError(f"Checkpoint {checkpoint_path} must contain 'model' and 'config' keys.")

        # Initialize the model
        model = HookedTransformer(ckpt["config"])
        model.load_state_dict(ckpt["model"])
        model.eval()
        print(f"Model loaded successfully from {checkpoint_path}")

    except Exception as e:
        print(f"Error loading model from {checkpoint_path}: {e}")
        return

    # 1. Visualize Input Embedding (W_E)
    try:
        W_E = model.embed.W_E.detach().cpu().numpy()
        print(f"Input Embedding Matrix (W_E) Shape: {W_E.shape}")
        plt.figure()
        sns.heatmap(normalize_matrix(W_E), cmap=colormap)
        plt.title("Input Embedding Matrix (W_E)")
        if save_plots:
            plt.savefig("input_embedding_matrix.png")
        if show_plots:
            plt.show()
        print(f"Sparsity of W_E: {calculate_sparsity(W_E):.2%}")
    except AttributeError:
        print("Input embedding matrix not found.")

    # 2. Visualize Output Embedding (W_U)
    try:
        W_U = model.unembed.W_U.detach().cpu().numpy()
        print(f"Output Embedding Matrix (W_U) Shape: {W_U.shape}")
        plt.figure()
        sns.heatmap(normalize_matrix(W_U), cmap=colormap)
        plt.title("Output Embedding Matrix (W_U)")
        if save_plots:
            plt.savefig("output_embedding_matrix.png")
        if show_plots:
            plt.show()
        print(f"Sparsity of W_U: {calculate_sparsity(W_U):.2%}")
    except AttributeError:
        print("Output embedding matrix not found.")

    # 3. Visualize Attention Weights (W_Q)
    try:
        W_Q = model.blocks[0].attn.W_Q.detach().cpu().numpy()
        print(f"Attention Weight Matrix (W_Q) Shape: {W_Q.shape}")
        for i in range(W_Q.shape[0]):
            plt.figure()
            sns.heatmap(normalize_matrix(W_Q[i]), cmap=colormap)
            plt.title(f"Attention W_Q (Head {i})")
            if save_plots:
                plt.savefig(f"attention_W_Q_head_{i}.png")
            if show_plots:
                plt.show()

        # Aggregated Attention Across Heads
        W_Q_mean = W_Q.mean(axis=0)
        plt.figure()
        sns.heatmap(normalize_matrix(W_Q_mean), cmap=colormap)
        plt.title("Aggregated Attention W_Q")
        if save_plots:
            plt.savefig("aggregated_attention_W_Q.png")
        if show_plots:
            plt.show()
        print(f"Sparsity of W_Q: {calculate_sparsity(W_Q):.2%}")
    except AttributeError:
        print("Attention matrices not found.")

    print("\nVisualization Complete.")

if __name__ == "__main__":
    # Path to the checkpoint file
    checkpoint_path = "/content/grokking_model.pth"

    # Call the function to analyze and visualize
    visualize_model_from_checkpoint(
        checkpoint_path=checkpoint_path,
        show_plots=True,
        save_plots=False,
        normalize=True,
        colormap="viridis",
    )

def visualize_and_log_with_tensorboard(
    model_path,
    log_dir="./logs",
    normalize=False,
    colormap="viridis",
):
    """
    Visualize and log model embeddings, attention weights, and training metrics using TensorBoard.
    """

    def normalize_matrix(matrix):
        """Normalize a matrix for visualization."""
        return matrix / matrix.max() if normalize else matrix

    def calculate_sparsity(matrix, threshold=1e-3):
        """Calculate the fraction of weights below a threshold."""
        return (np.abs(matrix) < threshold).mean()

    try:
        # Load the saved model and config
        ckpt = torch.load(model_path, map_location="cpu")
        if "model" not in ckpt or "config" not in ckpt:
            raise ValueError(f"Checkpoint {model_path} must contain 'model' and 'config' keys.")

        # Initialize the model
        model = HookedTransformer(ckpt["config"])
        model.load_state_dict(ckpt["model"])
        model.eval()
        print(f"Model loaded successfully from {model_path}")

    except Exception as e:
        print(f"Error loading model from {model_path}: {e}")
        return

    # Initialize TensorBoard writer
    writer = SummaryWriter(log_dir=log_dir)
    print(f"Logging data to TensorBoard at {log_dir}")

    # 1. Input Embedding (W_E)
    try:
        W_E = model.embed.W_E.detach().cpu().numpy()
        print(f"Input Embedding Matrix (W_E) Shape: {W_E.shape}")
        writer.add_histogram("Weights/Input Embedding (W_E)", W_E)
        writer.add_scalar("Sparsity/Input Embedding (W_E)", calculate_sparsity(W_E))
        plt.figure()
        sns.heatmap(normalize_matrix(W_E), cmap=colormap)
        plt.title("Input Embedding Matrix (W_E)")
        plt.savefig("input_embedding_matrix.png")  # Save for reference
        writer.add_image("Input Embedding Matrix (W_E)", plt.imread("input_embedding_matrix.png"), dataformats="HWC")
        plt.close()
    except AttributeError:
        print("Input embedding matrix not found.")

    # 2. Output Embedding (W_U)
    try:
        W_U = model.unembed.W_U.detach().cpu().numpy()
        print(f"Output Embedding Matrix (W_U) Shape: {W_U.shape}")
        writer.add_histogram("Weights/Output Embedding (W_U)", W_U)
        writer.add_scalar("Sparsity/Output Embedding (W_U)", calculate_sparsity(W_U))
        plt.figure()
        sns.heatmap(normalize_matrix(W_U), cmap=colormap)
        plt.title("Output Embedding Matrix (W_U)")
        plt.savefig("output_embedding_matrix.png")
        writer.add_image("Output Embedding Matrix (W_U)", plt.imread("output_embedding_matrix.png"), dataformats="HWC")
        plt.close()
    except AttributeError:
        print("Output embedding matrix not found.")

    # 3. Attention Weights (W_Q)
    try:
        W_Q = model.blocks[0].attn.W_Q.detach().cpu().numpy()
        print(f"Attention Weight Matrix (W_Q) Shape: {W_Q.shape}")
        writer.add_histogram("Weights/Attention Weights (W_Q)", W_Q)
        writer.add_scalar("Sparsity/Attention Weights (W_Q)", calculate_sparsity(W_Q))

        # Per-Head Visualization
        for i in range(W_Q.shape[0]):
            plt.figure()
            sns.heatmap(normalize_matrix(W_Q[i]), cmap=colormap)
            plt.title(f"Attention W_Q (Head {i})")
            plt.savefig(f"attention_W_Q_head_{i}.png")
            writer.add_image(f"Attention Head {i}", plt.imread(f"attention_W_Q_head_{i}.png"), dataformats="HWC")
            plt.close()

        # Aggregated Attention Across Heads
        W_Q_mean = W_Q.mean(axis=0)
        plt.figure()
        sns.heatmap(normalize_matrix(W_Q_mean), cmap=colormap)
        plt.title("Aggregated Attention W_Q")
        plt.savefig("aggregated_attention_W_Q.png")
        writer.add_image("Aggregated Attention W_Q", plt.imread("aggregated_attention_W_Q.png"), dataformats="HWC")
        plt.close()
    except AttributeError:
        print("Attention weight matrices not found.")

    # Additional Metrics (if available in checkpoint)
    try:
        train_losses = ckpt.get("train_losses", [])
        test_losses = ckpt.get("test_losses", [])
        for epoch, (train_loss, test_loss) in enumerate(zip(train_losses, test_losses)):
            writer.add_scalar("Loss/Train", train_loss, epoch)
            writer.add_scalar("Loss/Test", test_loss, epoch)

        print("Losses logged to TensorBoard.")
    except Exception as e:
        print(f"Error logging losses: {e}")

    writer.close()
    print("\nVisualization and logging complete.")



if __name__ == "__main__":
    # Path to the model checkpoint
    model_path = "/content/drive/MyDrive/grokking/grokking_model.pth"  # Update with your actual path

    # Call the function to analyze and visualize
    visualize_and_log_with_tensorboard(
        model_path=model_path,
        log_dir="./logs",
        normalize=True,
        colormap="viridis",
    )

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

# %tensorboard --logdir ./logs

def visualize_model_per_layer_and_head(
    model_path,
    log_dir="./logs_layers_heads",
    normalize=False,
    colormap="viridis",
):
    """
    Visualize embeddings and attention weights for each layer and head.
    Logs data to TensorBoard and saves plots for individual layers and heads.
    """

    def normalize_matrix(matrix):
        """Normalize a matrix for visualization."""
        return matrix / matrix.max() if normalize else matrix

    def calculate_sparsity(matrix, threshold=1e-3):
        """Calculate the fraction of weights below a threshold."""
        return (np.abs(matrix) < threshold).mean()

    # Load model
    try:
        ckpt = torch.load(model_path, map_location="cpu")
        if "model" not in ckpt or "config" not in ckpt:
            raise ValueError(f"Checkpoint must contain 'model' and 'config' keys.")
        model = HookedTransformer(ckpt["config"])
        model.load_state_dict(ckpt["model"])
        model.eval()
        print(f"Model loaded successfully from {model_path}")
    except Exception as e:
        print(f"Error loading model: {e}")
        return

    # Initialize TensorBoard writer
    writer = SummaryWriter(log_dir=log_dir)
    print(f"Logging data to TensorBoard at {log_dir}")

    # Input Embedding (W_E)
    W_E = model.embed.W_E.detach().cpu().numpy()
    writer.add_histogram("Weights/Input_Embedding (W_E)", W_E)
    plt.figure()
    sns.heatmap(normalize_matrix(W_E), cmap=colormap)
    plt.title("Input Embedding Matrix (W_E)")
    plt.savefig("input_embedding.png")
    plt.close()

    # Output Embedding (W_U)
    W_U = model.unembed.W_U.detach().cpu().numpy()
    writer.add_histogram("Weights/Output_Embedding (W_U)", W_U)
    plt.figure()
    sns.heatmap(normalize_matrix(W_U), cmap=colormap)
    plt.title("Output Embedding Matrix (W_U)")
    plt.savefig("output_embedding.png")
    plt.close()

    # Attention Weights (W_Q) per layer and head
    for layer_idx, block in enumerate(model.blocks):
        W_Q = block.attn.W_Q.detach().cpu().numpy()  # Shape: (n_heads, d_model, d_head)

        # Per Head
        for head_idx in range(W_Q.shape[0]):
            head_weights = W_Q[head_idx]
            writer.add_histogram(f"Layer_{layer_idx}/Head_{head_idx}/Attention_Weights (W_Q)", head_weights)
            plt.figure()
            sns.heatmap(normalize_matrix(head_weights), cmap=colormap)
            plt.title(f"Layer {layer_idx}, Head {head_idx}: Attention Weights (W_Q)")
            plt.savefig(f"layer_{layer_idx}_head_{head_idx}_attention_weights.png")
            plt.close()

        # Aggregated Across Heads
        W_Q_mean = W_Q.mean(axis=0)
        writer.add_histogram(f"Layer_{layer_idx}/Aggregated_Attention_Weights (W_Q)", W_Q_mean)
        plt.figure()
        sns.heatmap(normalize_matrix(W_Q_mean), cmap=colormap)
        plt.title(f"Layer {layer_idx}: Aggregated Attention Weights (W_Q)")
        plt.savefig(f"layer_{layer_idx}_aggregated_attention_weights.png")
        plt.close()

    writer.close()
    print("Visualization and logging complete.")


# Example usage
model_path = "./grokking_model.pth"  # Replace with your model path
visualize_model_per_layer_and_head(
    model_path=model_path,
    log_dir="./logs_layers_heads",
    normalize=True,
    colormap="viridis",
)

def analyze_checkpoint_evolution(
    checkpoint_paths,  # List of paths to checkpoints (e.g., early, mid, post-grokking)
    log_dir="./logs_evolution",
    normalize=False,
    colormap="viridis",
):
    """
    Analyze the evolution of model weights and sparsity across training checkpoints.
    Logs data to TensorBoard and saves comparison plots for embeddings and attention weights.
    """

    def normalize_matrix(matrix):
        """Normalize a matrix for visualization."""
        return matrix / matrix.max() if normalize else matrix

    def calculate_sparsity(matrix, threshold=1e-3):
        """Calculate the fraction of weights below a threshold."""
        return (np.abs(matrix) < threshold).mean()

    # Initialize TensorBoard writer
    writer = SummaryWriter(log_dir=log_dir)
    print(f"Logging data to TensorBoard at {log_dir}")

    # Analyze each checkpoint
    for idx, checkpoint_path in enumerate(checkpoint_paths):
        print(f"\nAnalyzing Checkpoint {idx + 1}: {checkpoint_path}")
        try:
            # Load the model and weights
            ckpt = torch.load(checkpoint_path, map_location="cpu")
            if "model" not in ckpt or "config" not in ckpt:
                raise ValueError(f"Checkpoint {checkpoint_path} must contain 'model' and 'config' keys.")
            model = HookedTransformer(ckpt["config"])
            model.load_state_dict(ckpt["model"])
            model.eval()

            # Input Embedding (W_E)
            W_E = model.embed.W_E.detach().cpu().numpy()
            writer.add_histogram(f"Checkpoint_{idx+1}/Weights/Input_Embedding (W_E)", W_E)
            writer.add_scalar(f"Checkpoint_{idx+1}/Sparsity/Input_Embedding (W_E)", calculate_sparsity(W_E))
            plt.figure()
            sns.histplot(W_E.flatten(), kde=True, color="blue", bins=50)
            plt.title(f"Checkpoint {idx + 1}: Input Embedding (W_E)")
            plt.savefig(f"checkpoint_{idx+1}_input_embedding.png")
            plt.close()

            # Output Embedding (W_U)
            W_U = model.unembed.W_U.detach().cpu().numpy()
            writer.add_histogram(f"Checkpoint_{idx+1}/Weights/Output_Embedding (W_U)", W_U)
            writer.add_scalar(f"Checkpoint_{idx+1}/Sparsity/Output_Embedding (W_U)", calculate_sparsity(W_U))
            plt.figure()
            sns.histplot(W_U.flatten(), kde=True, color="green", bins=50)
            plt.title(f"Checkpoint {idx + 1}: Output Embedding (W_U)")
            plt.savefig(f"checkpoint_{idx+1}_output_embedding.png")
            plt.close()

            # Attention Weights (W_Q)
            W_Q = model.blocks[0].attn.W_Q.detach().cpu().numpy()
            writer.add_histogram(f"Checkpoint_{idx+1}/Weights/Attention_Weights (W_Q)", W_Q)
            writer.add_scalar(f"Checkpoint_{idx+1}/Sparsity/Attention_Weights (W_Q)", calculate_sparsity(W_Q))
            plt.figure()
            sns.histplot(W_Q.flatten(), kde=True, color="red", bins=50)
            plt.title(f"Checkpoint {idx + 1}: Attention Weights (W_Q)")
            plt.savefig(f"checkpoint_{idx+1}_attention_weights.png")
            plt.close()

            # Aggregated Attention
            W_Q_mean = W_Q.mean(axis=0)
            plt.figure()
            sns.heatmap(normalize_matrix(W_Q_mean), cmap=colormap)
            plt.title(f"Checkpoint {idx + 1}: Aggregated Attention (W_Q)")
            plt.savefig(f"checkpoint_{idx+1}_aggregated_attention.png")
            plt.close()

        except Exception as e:
            print(f"Error analyzing checkpoint {idx + 1}: {e}")

    writer.close()
    print("\nAnalysis Complete.")

# Example usage:
checkpoint_paths = ["/content/drive/MyDrive/grokking/grokking_model.pth"]  # Use the actual path of your saved model


# Call the function to analyze and log checkpoint evolution
analyze_checkpoint_evolution(
    checkpoint_paths=checkpoint_paths,
    log_dir="./logs_evolution",
    normalize=True,
    colormap="viridis",
)

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

# %tensorboard --logdir ./logs

"""**Regularizaton**: Dropout"""

def train_dropout():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("[Dropout-Only] Running on device:", device)

    # 1) Create dataset
    train_data, train_labels, test_data, test_labels = make_mod113_dataset(device=device)

    # 2) Build config with dropout
    cfg = build_model_config(device)
    cfg.resid_p = 0.1
    cfg.attn_p  = 0.1
    cfg.ff_p    = 0.1

    # 3) Instantiate model
    model = HookedTransformer(cfg).to(device)
    freeze_biases(model)

    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1.0, betas=(0.9, 0.98))

    # 4) Training loop
    checkpoint_path = "dropout_model.pth"

    train_losses = []
    test_losses  = []

    for epoch in tqdm.tqdm(range(num_epochs)):
        model.train()
        logits_train = model(train_data)
        train_loss_val = loss_fn(logits_train, train_labels)
        train_loss_val.backward()
        optimizer.step()
        optimizer.zero_grad()
        train_losses.append(train_loss_val.item())

        # Evaluate
        model.eval()
        with torch.inference_mode():
            logits_test = model(test_data)
            test_loss_val = loss_fn(logits_test, test_labels)
            test_losses.append(test_loss_val.item())

        # checkpoint
        if (epoch + 1) % checkpoint_every == 0:
            torch.save({
                'epoch': epoch,
                'model': model.state_dict(),
                'optimizer': optimizer.state_dict(),
                'train_losses': train_losses,
                'test_losses': test_losses,
            }, checkpoint_path)
            print(f"[Dropout] Epoch {epoch} | Train: {train_loss_val:.4f} | Test: {test_loss_val:.4f}")

    # final save
    torch.save({
        'epoch': num_epochs,
        'model': model.state_dict(),
        'optimizer': optimizer.state_dict(),
        'train_losses': train_losses,
        'test_losses': test_losses,
    }, checkpoint_path)

    # 5) Use module's visualization
    visualize_losses(
        train_losses,
        test_losses,
        clamp=15,
        title="(a+b)%113 with Dropout (0.1)"
    )

if __name__ == "__main__":
    train_dropout()

"""**Regularization**: Label Smoothing"""

def label_smoothing_loss(model, data, labels, smoothing=0.1):
    """
    Implement cross-entropy with label smoothing for final token.
    """
    logits_3d = model(data.long())   # shape (batch, 2, p)
    logits = logits_3d[:, -1]        # shape (batch, p)
    p_val  = logits.size(-1)

    # Construct the smoothed distribution
    with torch.no_grad():
        true_dist = torch.zeros_like(logits).scatter_(-1, labels.unsqueeze(-1), 1.0)
        true_dist = true_dist * (1.0 - smoothing) + (smoothing / p_val)

    log_probs = torch.log_softmax(logits, dim=-1)
    loss = - (true_dist * log_probs).sum(dim=-1).mean()
    return loss

def train_label_smoothing():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("[LabelSmoothing-Only] Running on device:", device)

    train_data, train_labels, test_data, test_labels = make_mod113_dataset(device=device)
    cfg = build_model_config(device=device)
    model = HookedTransformer(cfg).to(device)
    freeze_biases(model)

    optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=1.0, betas=(0.9, 0.98))

    smoothing = 0.1

    checkpoint_path = "label_smoothing_model.pth"

    train_losses = []
    test_losses  = []

    for epoch in tqdm.tqdm(range(num_epochs)):
        model.train()
        train_loss_val = label_smoothing_loss(model, train_data, train_labels, smoothing=smoothing)
        train_loss_val.backward()
        optimizer.step()
        optimizer.zero_grad()
        train_losses.append(train_loss_val.item())

        # Evaluate
        model.eval()
        with torch.inference_mode():
            test_loss_val = label_smoothing_loss(model, test_data, test_labels, smoothing=smoothing)
            test_losses.append(test_loss_val.item())

        if (epoch + 1) % checkpoint_every == 0:
            torch.save({
                'epoch': epoch,
                'model': model.state_dict(),
                'optimizer': optimizer.state_dict(),
                'train_losses': train_losses,
                'test_losses': test_losses,
            }, checkpoint_path)
            print(f"[LabelSmoothing] Epoch {epoch} | Train: {train_loss_val:.4f} | Test: {test_loss_val:.4f}")

    # Final Save
    torch.save({
        'epoch': num_epochs,
        'model': model.state_dict(),
        'optimizer': optimizer.state_dict(),
        'train_losses': train_losses,
        'test_losses': test_losses,
    }, checkpoint_path)

    # Visualize using the module function
    visualize_losses(
        train_losses,
        test_losses,
        clamp=15,
        title="(a+b)%113 with Label Smoothing (0.1)"
    )

if __name__ == "__main__":
    train_label_smoothing()

"""**Regularization**: Gradient Clipping"""

def train_grad_clipping():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("[GradClipping-Only] Running on device:", device)

    train_data, train_labels, test_data, test_labels = make_mod113_dataset(device=device)
    cfg = build_model_config(device=device)
    model = HookedTransformer(cfg).to(device)
    freeze_biases(model)

    optimizer = AdamW(model.parameters(), lr=1e-3, weight_decay=1.0, betas=(0.9, 0.98))

    checkpoint_path = "grad_clipping_model.pth"
    clip_norm = 1.0  # example gradient norm

    train_losses = []
    test_losses  = []

    for epoch in tqdm.tqdm(range(num_epochs)):
        model.train()
        train_loss_val = loss_fn(model(train_data), train_labels)
        train_loss_val.backward()

        # Clip gradients
        nn_utils.clip_grad_norm_(model.parameters(), max_norm=clip_norm)

        optimizer.step()
        optimizer.zero_grad()
        train_losses.append(train_loss_val.item())

        # Evaluate
        model.eval()
        with torch.inference_mode():
            test_loss_val = loss_fn(model(test_data), test_labels)
            test_losses.append(test_loss_val.item())

        if (epoch + 1) % checkpoint_every == 0:
            torch.save({
                'epoch': epoch,
                'model': model.state_dict(),
                'optimizer': optimizer.state_dict(),
                'train_losses': train_losses,
                'test_losses': test_losses,
            }, checkpoint_path)
            print(f"[GradClipping] Epoch {epoch} | Train: {train_loss_val:.4f} | Test: {test_loss_val:.4f}")

    # Final Save
    torch.save({
        'epoch': num_epochs,
        'model': model.state_dict(),
        'optimizer': optimizer.state_dict(),
        'train_losses': train_losses,
        'test_losses': test_losses,
        'clip_norm': clip_norm,
    }, checkpoint_path)

    # Plot via the module
    visualize_losses(
        train_losses,
        test_losses,
        clamp=15,
        title="(a+b)%113 with Gradient Clipping"
    )

if __name__ == "__main__":
    train_grad_clipping()

"""study grokking with the changing the values for all the regularizations, like dropout at 1, 1.2, 1.4, 2, etc, to all the other regulalization methods above for grokking. Get the heatmaps too i have done above for the non regularized model. After that merge all the regularization methods into one model and also get it's heatmap. Do a tensorboard vizualization on them."""

import torch
import torch.nn as nn
import torch.optim as optim
import tqdm
import copy
import matplotlib.pyplot as plt
from transformer_lens import HookedTransformer, HookedTransformerConfig

# Constants
p = 113
lr = 1e-3
weight_decay = 1.0  # Setting weight decay to 1
num_epochs = 25000
checkpoint_every = 5000
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Dataset creation function
def make_mod113_dataset(device):
    torch.manual_seed(42)
    a = torch.arange(p).repeat_interleave(p)
    b = torch.arange(p).repeat(p)
    data = torch.stack([a, b, torch.full_like(a, p)], dim=1)
    labels = (a + b) % p

    split = int(0.7 * len(data))
    train_data, test_data = data[:split], data[split:]
    train_labels, test_labels = labels[:split], labels[split:]

    return train_data.to(device), train_labels.to(device), test_data.to(device), test_labels.to(device)

# Model configuration
def build_model_config(device):
    return HookedTransformerConfig(
        d_vocab=p + 1,  # Input vocabulary size
        d_vocab_out=p,  # Output classes
        n_ctx=3,        # Context length (a, b, and p)
        d_model=128,    # Embedding dimension
        n_layers=1,     # Number of transformer layers
        n_heads=4,      # Attention heads
        d_head=32,      # Dimension per attention head
        d_mlp=512,      # Hidden layer size of MLP
        act_fn="relu",  # Activation function
        device=device,
        seed=42,
        init_weights=True
    )

# Loss function
def loss_fn(logits, labels):
    if logits.dim() == 3:
        logits = logits[:, -1]  # Use logits from the last token
    return nn.CrossEntropyLoss()(logits, labels)

# Training loop
def train_model():
    # Create dataset
    train_data, train_labels, test_data, test_labels = make_mod113_dataset(device)

    # Build model
    cfg = build_model_config(device)
    model = HookedTransformer(cfg).to(device)

    # Freeze biases (optional)
    for name, param in model.named_parameters():
        if "b_" in name:
            param.requires_grad = False

    # Optimizer
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)

    # Training loop
    train_losses = []
    test_losses = []
    checkpoints = []
    checkpoint_epochs = []

    for epoch in tqdm.tqdm(range(num_epochs)):
        # Training step
        model.train()
        logits_train = model(train_data)
        train_loss = loss_fn(logits_train, train_labels)
        optimizer.zero_grad()
        train_loss.backward()
        optimizer.step()
        train_losses.append(train_loss.item())

        # Evaluation step
        model.eval()
        with torch.no_grad():
            logits_test = model(test_data)
            test_loss = loss_fn(logits_test, test_labels)
            test_losses.append(test_loss.item())

        # Checkpoint
        if (epoch + 1) % checkpoint_every == 0:
            checkpoint_epochs.append(epoch)
            checkpoints.append(copy.deepcopy(model.state_dict()))
            print(f"Epoch {epoch + 1}: Train Loss = {train_loss.item():.4f}, Test Loss = {test_loss.item():.4f}")

    # Save model and checkpoints
    torch.save({
        "model": model.state_dict(),
        "config": model.cfg,
        "checkpoints": checkpoints,
        "checkpoint_epochs": checkpoint_epochs,
        "train_losses": train_losses,
        "test_losses": test_losses
    }, "./grokking_model_with_wd1.pth")
    print("Training complete. Model saved to ./grokking_model_with_wd1.pth")

    return train_losses, test_losses, checkpoint_epochs

# Visualization function
def visualize_losses(train_losses, test_losses):
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label="Train Loss", color="blue")
    plt.plot(test_losses, label="Test Loss", color="orange")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.title("Training and Test Losses Over Epochs")
    plt.legend()
    plt.grid(True)
    plt.show()

# Main function
if __name__ == "__main__":
    train_losses, test_losses, checkpoint_epochs = train_model()
    visualize_losses(train_losses, test_losses)

if __name__ == "__main__":
    main()